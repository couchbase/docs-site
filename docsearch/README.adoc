= How to Update the Search Index
:url-algolia: https://www.algolia.com/doc/guides/getting-started/what-is-algolia/
:url-docsearch-scraper-repo: https://github.com/algolia/docsearch-scraper
:url-docsearch-scraper-docs: https://community.algolia.com/docsearch/run-your-own.html
:url-config: https://github.com/couchbase/docs-site/raw/master/docsearch/production-config.json
:url-chromedriver: https://sites.google.com/a/chromium.org/chromedriver/
:url-pipenv: https://pipenv.readthedocs.io/en/latest/

This guide contains information for managing the search index for the Couchbase documentation site.
It covers where the search index lives and how to update it.
This procedure described here is used by the CI job defined by the Jenkinsfile in this directory.
This document is helpful to understand how the CI job works, or how to perform the update manually, if necessary.

== Overview

The search index for the documentation is hosted by {url-algolia}[Algolia].
The index, named *prod_docs_couchbase*, is stored in the Couchbase Algolia account.
The index is populated by the {url-docsearch-scraper-repo}[docsearch scraper] (aka crawler).

The sections below document the prerequisites for running the docsearch scraper, how to run the docsearch scraper to update the index, and how to activate the index once scraping is complete.

== Prerequisites

* git (to clone the docsearch-scraper repository)
* {url-pipenv}[pipenv] (to manage a local Python installation and packages)
* Chrome/Chromium and {url-chromedriver}[chromedriver] (or Docker)

== Setup

To begin, clone the {url-docsearch-scraper-repo} repository using git.

[subs=attributes+]
 $ git clone {url-docsearch-scraper-repo} &&
   cd "`basename $_`"

Next, create an [.path]_.env_ file in the cloned repository to define the application ID (`APPLICATION_ID`) and write API key (`API_KEY`).
_To protect the API key, only the final four characters of the API key are shown here._

----
APPLICATON_ID=NI1G57N08Q
API_KEY=****************************67dd
----

IMPORTANT: The API key used in this file is different than the one used for searching.
In the Algolia dashboard, it's labeled as the Write API Key.

The next step is to set up the Python environment and install the required packages.

 $ pipenv install && pipenv shell

If you don't plan to use the Docker image, you'll need to install both Chrome (or Chromium) and {url-chromedriver}[chromedriver].
Run the following command to make sure Chromedriver is installed successfully:

 $ chromedriver --version

Finally, you'll need the docsearch configuration file.
This configuration file is located in the root of the playbook repository for the Couchbase document.
Download the file from {url-config} and save it to the cloned repository.

You're now ready to run the scraper.

== Usage

There are three ways to run the scraper:

* docsearch run (uses local packages and chromedriver)
* docsearch docker:run (uses local packages and provided Docker image)
* docker run (uses provided Docker image)

WARNING: Rebuilding the index takes about 30 minutes because it has to visit every page in the site.

=== docsearch run

To update the index, pass the config file to the `docsearch run` command:

 $ ./docsearch run production-config.json

If that succeeds, skip to <<Activate Index>>.

If that command fails, you may need to run it in the provided Docker container.

=== docsearch docker:run

First, make sure you have Docker running on your machine and that you can list images.

 $ docker images

Then, run the `docsearch` command again, but use the Docker container instead:

 $ ./docsearch docker:run production-config.json

Now proceed to <<Activate Index>>.

=== docker run

Using Docker, it's possible to bypass the use of pipenv by invoking `docker run` directly.
First, create a script named `scrape` with the following contents:

.scrape
----
#!/usr/bin/bash

source .env

docker run \
  -e APPLICATION_ID=$APPLICATION_ID \
  -e API_KEY=$API_KEY \
  -e CONFIG="`cat ${1:-config.json}`" \
  -t --rm algolia/docsearch-scraper \
  /root/run
----

Then, make it executable:

 $ chmod 755 scrape

Finally, run it, passing the configuration file as the first argument:

 $ ./scrape production-config.json

Proceed to <<Activate Index>> to learn how to make the index live.

== Activate Index

To avoid clobbering the production index by an accidental run, the scraper is configured to write to a temporary index.
You'll need to run a script to move it into place.

First, install the docsearch package for Node.

 $ yarn add algoliasearch

Next, create the script `activate-production-index.js` in the current folder and populate it with the following contents:

.activate-production-index.js
[source,js]
----
const algoliasearch = require('algoliasearch')
const fs = require('fs')

;(async () => {
  const env = fs.readFileSync('.env', 'utf-8')
  const { APPLICATION_ID: appId, API_KEY: apiKey } = env.trim().split('\n').reduce((vars, line) => {
    const [ k, v ] = line.split('=')
    vars[k] = v
    return vars
  }, {})
  const algoliaclient = algoliasearch(appId, apiKey)
  await algoliaclient.deleteIndex('prod_docs_couchbase')
  await algoliaclient.moveIndex('next_docs_couchbase', 'prod_docs_couchbase')
})()
----

Finally, run the script using the following command:

 $ node activate-production-index.js

This script performs the following steps:

. Deletes the *prod_docs_couchbase* index.
. Renames the *next_docs_couchbase* index to *prod_docs_couchbase*.

TIP: If you want to avoid this step, simply change the index name at the top of the docsearch configuration file to *prod_docs_couchbase*.
The scraper already writes to a temporary index, then swaps it into place when it's finished.

The search index is now updated.

== See Also

* {url-docsearch-scraper-docs}[Official documentation for the docsearch scraper]
